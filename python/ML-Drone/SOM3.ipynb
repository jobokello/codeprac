{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
    "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "\n",
    "# for unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal\n",
    "from numpy.testing import assert_array_equal\n",
    "import unittest\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=None, neighborhood_function='gaussian',\n",
    "                 random_seed=None):\n",
    "        \"\"\"Initializes a Self Organizing Maps.\n",
    "        Parameters\n",
    "        ----------\n",
    "        decision_tree : decision tree\n",
    "        The decision tree to be exported.\n",
    "        x : int\n",
    "            x dimension of the SOM\n",
    "        y : int\n",
    "            y dimension of the SOM\n",
    "        input_len : int\n",
    "            Number of the elements of the vectors in input.\n",
    "        sigma : float, optional (default=1.0)\n",
    "            Spread of the neighborhood function, needs to be adequate\n",
    "            to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T)\n",
    "            where T is #num_iteration/2)\n",
    "            learning_rate, initial learning rate\n",
    "            (at the iteration t we have\n",
    "            learning_rate(t) = learning_rate / (1 + t/T)\n",
    "            where T is #num_iteration/2)\n",
    "        decay_function : function (default=None)\n",
    "            Function that reduces learning_rate and sigma at each iteration\n",
    "            default function:\n",
    "            lambda x, current_iteration, max_iter :\n",
    "                        x/(1+current_iteration/max_iter)\n",
    "        neighborhood_function : function, optional (default='gaussian')\n",
    "            Function that weights the neighborhood of a position in the map\n",
    "            possible values: 'gaussian', 'mexican_hat'\n",
    "        random_seed : int, optiona (default=None)\n",
    "            Random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "        if random_seed:\n",
    "            self._random_generator = random.RandomState(random_seed)\n",
    "        else:\n",
    "            self._random_generator = random.RandomState(random_seed)\n",
    "        if decay_function:\n",
    "            self._decay_function = decay_function\n",
    "        else:\n",
    "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
    "        self._learning_rate = learning_rate\n",
    "        self._sigma = sigma\n",
    "        # random initialization\n",
    "        self._weights = self._random_generator.rand(x, y, input_len)*2-1\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                # normalization\n",
    "                norm = fast_norm(self._weights[i, j])\n",
    "                self._weights[i, j] = self._weights[i, j] / norm\n",
    "        self._activation_map = zeros((x, y))\n",
    "        self._neigx = arange(x)\n",
    "        self._neigy = arange(y)  # used to evaluate the neighborhood function\n",
    "        neig_functions = {'gaussian': self._gaussian,\n",
    "                          'mexican_hat': self._mexican_hat}\n",
    "        if neighborhood_function not in neig_functions:\n",
    "            msg = '%s not supported. Functions available: %s'\n",
    "            raise ValueError(msg % (neighborhood_function,\n",
    "                                    ', '.join(neig_functions.keys())))\n",
    "        self.neighborhood = neig_functions[neighborhood_function]\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Returns the weights of the neural network\"\"\"\n",
    "        return self._weights\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\"Updates matrix activation_map, in this matrix\n",
    "           the element i,j is the response of the neuron i,j to x\"\"\"\n",
    "        s = subtract(x, self._weights)  # x - w\n",
    "        it = nditer(self._activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # || x - w ||\n",
    "            self._activation_map[it.multi_index] = fast_norm(s[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Returns the activation map to x\"\"\"\n",
    "        self._activate(x)\n",
    "        return self._activation_map\n",
    "\n",
    "    def _gaussian(self, c, sigma):\n",
    "        \"\"\"Returns a Gaussian centered in c\"\"\"\n",
    "        d = 2*pi*sigma*sigma\n",
    "        ax = exp(-power(self._neigx-c[0], 2)/d)\n",
    "        ay = exp(-power(self._neigy-c[1], 2)/d)\n",
    "        return outer(ax, ay)  # the external product gives a matrix\n",
    "\n",
    "    def _mexican_hat(self, c, sigma):\n",
    "        \"\"\"Mexican hat centered in c\"\"\"\n",
    "        xx, yy = meshgrid(self._neigx, self._neigy)\n",
    "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
    "        d = 2*pi*sigma*sigma\n",
    "        return exp(-p/d)*(1-2/d*p)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\"Computes the coordinates of the winning neuron for the sample x\"\"\"\n",
    "        self._activate(x)\n",
    "        return unravel_index(self._activation_map.argmin(),\n",
    "                             self._activation_map.shape)\n",
    "\n",
    "    def update(self, x, win, t):\n",
    "        \"\"\"Updates the weights of the neurons.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Current pattern to learn\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            Iteration index\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self._learning_rate, t, self.T)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, self.T)\n",
    "        # improves the performances\n",
    "        g = self.neighborhood(win, sig)*eta\n",
    "        it = nditer(g, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # eta * neighborhood_function * (x-w)\n",
    "            x_w = (x - self._weights[it.multi_index])\n",
    "            self._weights[it.multi_index] += g[it.multi_index] * x_w\n",
    "            # normalization\n",
    "            norm = fast_norm(self._weights[it.multi_index])\n",
    "            self._weights[it.multi_index] = self._weights[it.multi_index]/norm\n",
    "            it.iternext()\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\"Assigns a code book (weights vector of the winning neuron)\n",
    "        to each sample in data.\"\"\"\n",
    "        q = zeros(data.shape)\n",
    "        for i, x in enumerate(data):\n",
    "            q[i] = self._weights[self.winner(x)]\n",
    "        return q\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\"Initializes the weights of the SOM\n",
    "        picking random samples from data\"\"\"\n",
    "        it = nditer(self._activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            rand_i = self._random_generator.randint(len(data))\n",
    "            self._weights[it.multi_index] = data[rand_i]\n",
    "            norm = fast_norm(self._weights[it.multi_index])\n",
    "            self._weights[it.multi_index] = self._weights[it.multi_index]/norm\n",
    "            it.iternext()\n",
    "\n",
    "    def train_random(self, data, num_iteration):\n",
    "        \"\"\"Trains the SOM picking samples at random from data\"\"\"\n",
    "        self._init_T(num_iteration)\n",
    "        for iteration in range(num_iteration):\n",
    "            # pick a random sample\n",
    "            rand_i = self._random_generator.randint(len(data))\n",
    "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
    "\n",
    "    def train_batch(self, data, num_iteration):\n",
    "        \"\"\"Trains using all the vectors in data sequentially\"\"\"\n",
    "        self._init_T(len(data)*num_iteration)\n",
    "        iteration = 0\n",
    "        while iteration < num_iteration:\n",
    "            idx = iteration % (len(data)-1)\n",
    "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    def _init_T(self, num_iteration):\n",
    "        \"\"\"Initializes the parameter T needed to adjust the learning rate\"\"\"\n",
    "        # keeps the learning rate nearly constant\n",
    "        # for the last half of the iterations\n",
    "        self.T = num_iteration/2\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\"Returns the distance map of the weights.\n",
    "        Each cell is the normalised sum of the distances between\n",
    "        a neuron and its neighbours.\"\"\"\n",
    "        um = zeros((self._weights.shape[0], self._weights.shape[1]))\n",
    "        it = nditer(um, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
    "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
    "                    if (ii >= 0 and ii < self._weights.shape[0] and\n",
    "                            jj >= 0 and jj < self._weights.shape[1]):\n",
    "                        w_1 = self._weights[ii, jj, :]\n",
    "                        w_2 = self._weights[it.multi_index]\n",
    "                        um[it.multi_index] += fast_norm(w_1-w_2)\n",
    "            it.iternext()\n",
    "        um = um/um.max()\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        a = zeros((self._weights.shape[0], self._weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"Returns the quantization error computed as the average\n",
    "        distance between each input sample and its best matching unit.\"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            error += fast_norm(x-self._weights[self.winner(x)])\n",
    "        return error/len(data)\n",
    "\n",
    "    def win_map(self, data):\n",
    "        \"\"\"Returns a dictionary wm where wm[(i,j)] is a list\n",
    "        with all the patterns that have been mapped in the position i,j.\"\"\"\n",
    "        winmap = defaultdict(list)\n",
    "        for x in data:\n",
    "            winmap[self.winner(x)].append(x)\n",
    "        return winmap\n",
    "\n",
    "\n",
    "class TestMinisom(unittest.TestCase):\n",
    "    def setup_method(self, method):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                # checking weights normalization\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som._weights[i, j]))\n",
    "        self.som._weights = zeros((5, 5))  # fake weights\n",
    "        self.som._weights[2, 3] = 5.0\n",
    "        self.som._weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_unavailable_neigh_function(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            MiniSom(5, 5, 1, neighborhood_function='boooom')\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som._gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([5.0, 2.0])\n",
    "        assert winners[(2, 3)][0] == 5.0\n",
    "        assert winners[(1, 1)][0] == 2.0\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([5.0, 2.0])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        self.som.quantization_error([5, 2]) == 0.0\n",
    "        self.som.quantization_error([4, 1]) == 0.5\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([4, 2]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        # same initialization\n",
    "        assert_array_almost_equal(som1._weights, som2._weights)\n",
    "        data = random.rand(100, 2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data, 10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data, 10)\n",
    "        # same state after training\n",
    "        assert_array_almost_equal(som1._weights, som2._weights)\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som._weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
